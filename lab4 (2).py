# -*- coding: utf-8 -*-
"""Lab4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16odCRD5oAMyf6ILmCvdp7NuDglNELHwX
"""

# Task 1:
#Q:
# Are we dealing with supervised or unsupervised learning?
#A: we are dealing with supervised learning since 2 specific class in which we trained our data to

#Codes:
#Defining the input data to train the model

x_train=["This was awesome an awesome movie", 
         "Great movie! I liked it a lot", 
         "Happy ending! awesome acting by the hero",
         "loved it truly great",
        "bad not up to the mark",
         "could have been better",
         "surely a disappointing movie"
        ]
#1 for positive, 0 for negative
y_train=[1,1,1,1,0,0,0]

#to test our trained data
x_test=["I was happy and happy, I loved the acting in the movie", 
        "The movie I saw is bad", "I don't know if I like it or not, Its okay", "the movie is mysterious and exciting", "I don't like it at all"]

# Task 2:
# Do the necessary imports and instantiations:

from nltk.tokenize import RegexpTokenizer

from nltk.stem.porter import PorterStemmer

from nltk.corpus import stopwords

import nltk

nltk.download('stopwords')

import nltk
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer


# # In case of import errors
! pip install nltk
! pip install textblob
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import re
import nltk
import string
from nltk.corpus import stopwords

tokenizer=RegexpTokenizer(r'\w+')
en_stopwords=set(stopwords.words('english'))
ps=PorterStemmer()


#Q: What is a token for us?
#A: our token is a word


# Task 3:
# Define a function that cleans our text:

stop_words = stopwords.words("english")
wordnet = WordNetLemmatizer()
def clean_text(i):
  outl = []
  for x in i:
    x = x.lower()
    x = ' '.join([word for word in x.split(' ') if word not in stop_words])
    x = x.encode('ascii', 'ignore').decode()
    x = re.sub(r'https*\S+', ' ', x)
    x = re.sub(r'@\S+', ' ', x)
    x = re.sub(r'#\S+', ' ', x)
    x = re.sub(r'\'\w+', '', x)
    x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)
    x = re.sub(r'\w*\d+\w*', '', x)
    x = re.sub(r'\s{2,}', ' ', x)
    outl.append(x)
  return outl



# Task 4:
# Call your function cleanText to clean x_train and x_test. Name them x_clean and xt_clean.
x_clean = clean_text(x_train) 
xt_clean = clean_text(x_test) 


print(x_clean)
print(xt_clean)

# Task 5:
# The goal of this task is to vectorize your text.

from sklearn.feature_extraction.text import CountVectorizer

cv=CountVectorizer(ngram_range=(1,2))

x_vec=cv.fit_transform(x_clean).toarray()

x_vec

# Task 6:
# Add the following piece of code and try to understand the vectorization

cv.get_feature_names()

# Task 7:
# Similarly, transform xt_clean

xt_vec=cv.transform(xt_clean).toarray()

#Task 8:
#Import the naive bayes classifier and use it:
from sklearn.naive_bayes import MultinomialNB
#this version of naive bayes is used for text classification

mn=MultinomialNB()

mn.fit(x_vec,y_train)

y_pred=mn.predict(xt_vec)

#Print out y_pred and try to understand its meaning
print(y_pred)